#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Refactored visualization script using modular components.
Works with both ZOD and Waymo datasets.
"""
import os
import sys
import json
import argparse
import numpy as np
import torch

from core.model_builder import ModelBuilder
from core.data_loader import DataLoader as InferenceDataLoader
from core.visualizer import Visualizer
from utils.helpers import get_model_path, get_annotation_path
from integrations.visualization_uploader import (
    upload_all_visualizations_for_image,
    get_epoch_uuid_from_model_path
)


def calculate_num_classes(config):
    """
    Calculate number of unique classes after relabeling.
    
    Returns the count of unique training_index values in the config,
    which represents the actual number of output classes for the model
    after any class merging/relabeling.
    """
    unique_indices = set(cls['training_index'] for cls in config['Dataset']['classes'])
    return len(unique_indices)


def load_image_paths(path_arg, dataroot):
    """Load image paths from file or single path."""
    if path_arg.endswith(('.png', '.jpg', '.jpeg')):
        # Single image
        return [path_arg]
    else:
        # Text file with multiple paths
        with open(path_arg, 'r') as f:
            paths = f.read().splitlines()
        return paths


def get_lidar_path(cam_path, dataset_name):
    """Get LiDAR path based on dataset."""
    if dataset_name == 'zod':
        return cam_path.replace('/camera', '/lidar_png')
    else:  # waymo
        return cam_path.replace('/camera/', '/lidar_png/')


def prepare_model_inputs(rgb, lidar, modality):
    """Prepare model inputs based on modality."""
    if modality == 'rgb':
        return rgb, rgb
    elif modality == 'lidar':
        return lidar, lidar
    else:  # cross_fusion
        return rgb, lidar


def process_images(model, data_loader, visualizer, image_paths, dataroot, 
                   modality, dataset_name, device, config, epoch_uuid=None, upload=False):
    """Process and visualize all images."""
    model.eval()
    
    uploaded_count = 0
    failed_count = 0
    
    for idx, path in enumerate(image_paths, 1):
        # Construct full paths
        if os.path.isabs(path):
            cam_path = path
        else:
            cam_path = os.path.join(dataroot, path)
        
        anno_path = get_annotation_path(cam_path, dataset_name, config)
        lidar_path = get_lidar_path(cam_path, dataset_name)
        
        # Verify paths match
        rgb_name = os.path.basename(cam_path).split('.')[0]
        anno_name = os.path.basename(anno_path).split('.')[0]
        lidar_name = os.path.basename(lidar_path).split('.')[0]
        
        assert rgb_name == anno_name, f"RGB and annotation names don't match: {rgb_name} vs {anno_name}"
        assert rgb_name == lidar_name, f"RGB and LiDAR names don't match: {rgb_name} vs {lidar_name}"
        
        # Load data
        print(f'Processing image {idx}/{len(image_paths)}: {rgb_name}')
        rgb = data_loader.load_rgb(cam_path).to(device, non_blocking=True).unsqueeze(0)
        
        # Only load LiDAR data if not in RGB-only mode
        if modality == 'rgb':
            lidar = rgb  # Use RGB as dummy LiDAR input to avoid unnecessary loading
        else:
            lidar = data_loader.load_lidar(lidar_path).to(device, non_blocking=True).unsqueeze(0)
        
        # Run inference
        with torch.no_grad():
            rgb_input, lidar_input = prepare_model_inputs(rgb, lidar, modality)
            _, output_seg = model(rgb_input, lidar_input, modality)
        
        # Visualize
        visualizer.visualize_prediction(output_seg, cam_path, anno_path, idx)
        
        # Upload to Vision API if enabled
        if upload and epoch_uuid:
            print(f'Uploading visualizations for {rgb_name}...')
            image_filename = os.path.basename(cam_path)
            results = upload_all_visualizations_for_image(
                epoch_uuid=epoch_uuid,
                output_base=visualizer.output_base,
                image_name=image_filename
            )
            
            # Count successes
            success_count = sum(1 for r in results.values() if r is not None)
            if success_count > 0:
                uploaded_count += 1
                print(f'Successfully uploaded {success_count}/{len(results)} visualization types')
            else:
                failed_count += 1
                print(f'Failed to upload visualizations for {rgb_name}')
    
    print(f'\nCompleted visualization of {len(image_paths)} images')
    if upload:
        print(f'Upload summary: {uploaded_count} images uploaded, {failed_count} failed')


def main():
    # Parse arguments
    parser = argparse.ArgumentParser(description='Visualize Model Predictions (Refactored)')
    parser.add_argument('-p', '--path', type=str, required=False,
                       default='zod_dataset/visualizations.txt',
                       help='Path to image file or text file with image paths')
    parser.add_argument('-c', '--config', type=str, required=True,
                       help='Path to config file')
    parser.add_argument('--upload', action='store_true',
                       help='Upload visualizations to Vision API')
    parser.add_argument('--epoch-uuid', type=str, default=None,
                       help='Epoch UUID for upload (auto-detected from model if not provided)')
    args = parser.parse_args()
    
    # Load configuration
    with open(args.config, 'r') as f:
        config = json.load(f)
    
    # Fix default path for waymo
    dataset_name = config['Dataset']['name']
    if args.path == 'zod_dataset/visualizations.txt' and dataset_name == 'waymo':
        args.path = 'waymo_dataset/visualizations.txt'
    
    # Setup device
    device = torch.device(config['General']['device']
                         if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")
    
    # Get model path
    model_path = get_model_path(config)
    if not model_path:
        print("No model checkpoint found!")
        sys.exit(1)
    
    print(f"Using model: {model_path}")
    config['General']['model_path'] = model_path
    
    # Extract epoch UUID if uploading
    epoch_uuid = args.epoch_uuid
    if args.upload and not epoch_uuid:
        epoch_uuid = get_epoch_uuid_from_model_path(model_path)
        if epoch_uuid:
            print(f"Auto-detected epoch UUID: {epoch_uuid}")
        else:
            print("Warning: Could not auto-detect epoch UUID from model path")
            print("Upload will be skipped unless --epoch-uuid is provided")
            args.upload = False
    
    if args.upload and epoch_uuid:
        print(f"Visualizations will be uploaded to Vision API for epoch: {epoch_uuid}")
    
    # Extract config name for output directory
    dataset_name = config['Dataset']['name']
    log_dir = config['Log']['logdir']
    output_base = os.path.join(log_dir, 'visualizations')
    
    # Build and load model
    model_builder = ModelBuilder(config, device)
    model = model_builder.build_model()
    model, _ = model_builder.load_checkpoint(model, model_path)
    model.eval()
    
    # Setup data loader
    data_loader = InferenceDataLoader(config)
    
    # Setup visualizer
    visualizer = Visualizer(config, output_base)
    
    # Load image paths
    dataroot = os.path.abspath(config['Dataset']['dataset_root'])
    image_paths = load_image_paths(args.path, dataroot)
    print(f"Found {len(image_paths)} images to process")
    
    # Get modality
    modality = config['CLI']['mode']
    print(f"Using modality: {modality}")
    
    # Process images
    process_images(
        model, data_loader, visualizer, image_paths, 
        dataroot, modality, dataset_name, device, config,
        epoch_uuid=epoch_uuid, upload=args.upload
    )


if __name__ == '__main__':
    main()
